
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import *
from awsglue.job import Job
import boto3
import pandas as pd
from  pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql.functions import *


glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session
job = Job(glueContext)
sqlContext = SQLContext(spark)



 for table_obj in sqlContext.tables('rds-oracle-database').select("tableName").collect():
        tablename = print ('\n-- tableName: '+table)



response = client.start_job_run(
             JOBNAME = 'RDS_ORACLE_S3',
             Arguments = {
               '--DB_NAME':   'rds-oracle-databas',
               '--target_format':  'csv',
               '--aws_region':  'ap-southeast-1',
               '--save_location': 's3://tipper-s3/'
               '--outputFileName' =  'save_location + "/" + table'
               '--table_name' =  tablename} )


args = getResolvedOptions(sys.argv,
                          ['JOB_NAME',
                           'DB_NAME',
                           'target_format',
                           'aws_region',
                           'save_location',
                           'outputFileName',
                           'table_name'])

job.init(args['JOB_NAME'], args)

for table in table_name:
    datasource0 = glueContext.create_dynamic_frame.from_catalog(database = DB_NAME, table_name = args['table_name', transformation_ctx = "datasource0")
    datasink4 = glueContext.write_dynamic_frame.from_options(frame = datasource0, connection_type = "s3", connection_options = {"path": "s3://tipper-s3/"+ table + "/"}, format = "parquet", transformation_ctx = "datasink4")

job.comit()    


